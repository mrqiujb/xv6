xv6 is a re-implementation of Dennis Ritchie's and Ken Thompson's Unix
Version 6 (v6).  xv6 loosely follows the structure and style of v6,
but is implemented for a modern RISC-V multiprocessor using ANSI C.

ACKNOWLEDGMENTS

xv6 is inspired by John Lions's Commentary on UNIX 6th Edition (Peer
to Peer Communications; ISBN: 1-57398-013-7; 1st edition (June 14,
2000)). See also https://pdos.csail.mit.edu/6.828/, which
provides pointers to on-line resources for v6.

The following people have made contributions: Russ Cox (context switching,
locking), Cliff Frey (MP), Xiao Yu (MP), Nickolai Zeldovich, and Austin
Clements.

We are also grateful for the bug reports and patches contributed by
Silas Boyd-Wickizer, Anton Burtsev, Dan Cross, Cody Cutler, Mike CAT,
Tej Chajed, Asami Doi, eyalz800, , Nelson Elhage, Saar Ettinger, Alice
Ferrazzi, Nathaniel Filardo, Peter Froehlich, Yakir Goaron,Shivam
Handa, Bryan Henry, jaichenhengjie, Jim Huang, Alexander Kapshuk,
Anders Kaseorg, kehao95, Wolfgang Keller, Jonathan Kimmitt, Eddie
Kohler, Austin Liew, Imbar Marinescu, Yandong Mao, Matan Shabtay,
Hitoshi Mitake, Carmi Merimovich, Mark Morrissey, mtasm, Joel Nider,
Greg Price, Ayan Shafqat, Eldar Sehayek, Yongming Shen, Fumiya
Shigemitsu, Takahiro, Cam Tenny, tyfkda, Rafael Ubal, Warren Toomey,
Stephen Tu, Pablo Ventura, Xi Wang, Keiichi Watanabe, Nicolas
Wolovick, wxdao, Grant Wu, Jindong Zhang, Icenowy Zheng, and Zou Chang
Wei.

The code in the files that constitute xv6 is
Copyright 2006-2020 Frans Kaashoek, Robert Morris, and Russ Cox.

ERROR REPORTS

Please send errors and suggestions to Frans Kaashoek and Robert Morris
(kaashoek,rtm@mit.edu). The main purpose of xv6 is as a teaching
operating system for MIT's 6.S081, so we are more interested in
simplifications and clarifications than new features.

BUILDING AND RUNNING XV6

You will need a RISC-V "newlib" tool chain from
https://github.com/riscv/riscv-gnu-toolchain, and qemu compiled for
riscv64-softmmu. Once they are installed, and in your shell
search path, you can run "make qemu".
## Lab: Multithreading

​	This lab will familiarize you with multithreading. You will implement switching between threads in a user-level threads package, use multiple threads to speed up a program, and implement a barrier.

​	本次实验将会是你熟悉多线程，你将会实现一个用户级别的线程切换，使用多线程加速程序，并且实现一个屏障。

​	

### Uthread: switching between threads ([moderate](https://pdos.csail.mit.edu/6.828/2022/labs/guidance.html))

​	In this exercise you will design the context switch mechanism for a user-level threading system, and then implement it. To get you started, your xv6 has two files user/uthread.c and user/uthread_switch.S, and a rule in the Makefile to build a uthread program. uthread.c contains most of a user-level threading package, and code for three simple test threads. The threading package is missing some of the code to create a thread and to switch between threads.

​	在这儿练习中，你将会为用户及线程设计一个上下文切换的机制，并且实现它。为了让你开始，你的xv6项目有两个文件，一个user/uthread.c一个user/uthread_switch.S，并且Makefile里面有构建这些代码。uthread.c里面包含大多数的用户级的线程的包，并且有三个简单的测试代码。这个线程包丢失了一些创建和切换线程的代码。

​	Your job is to come up with a plan to create threads and save/restore registers to switch between threads, and implement that plan. When you're done, `make grade` should say that your solution passes the `uthread` test.

​	你的工作是计划去创建线程保存/恢复寄存器用来切换两个线程，并且实现它。当你做完之后，make grade应该告诉你你的解决方案已经通过了uthread的测试。

​	Once you've finished, you should see the following output when you run `uthread` on xv6 (the three threads might start in a different order):

​	一旦你完成，你应该如下输出当你运行uthread在xv6系统上。

​	This output comes from the three test threads, each of which has a loop that prints a line and then yields the CPU to the other threads.

​	这个输出来自三个测试线程，每个线程都有一个循环，打印一行，然后将CPU分配给其他线程。

At this point, however, with no context switch code, you'll see no output.

​	然而，在这一点上，如果没有上下文切换代码，您将看不到任何输出。

​	You will need to add code to `thread_create()` and `thread_schedule()` in `user/uthread.c`, and `thread_switch` in `user/uthread_switch.S`. One goal is ensure that when `thread_schedule()` runs a given thread for the first time, the thread executes the function passed to `thread_create()`, on its own stack. Another goal is to ensure that `thread_switch` saves the registers of the thread being switched away from, restores the registers of the thread being switched to, and returns to the point in the latter thread's instructions where it last left off. You will have to decide where to save/restore registers; modifying `struct thread` to hold registers is a good plan. You'll need to add a call to `thread_switch` in `thread_schedule`; you can pass whatever arguments you need to `thread_switch`, but the intent is to switch from thread `t` to `next_thread`.

​	您需要将代码添加到“user/uthread.c”中的“thread_create（）”和“thread_schedule（）”，以及“user/uuthread_switch.S”中的‘thread_switch’。一个目标是确保当“thread_schedule”（）首次运行给定线程时，线程在自己的堆栈上执行传递给“thread_create（（）”的函数。另一个目标是确保“thread_switch”保存要切换的线程的寄存器，恢复要切换到的线程的注册表，并返回到后一个线程的指令中上次停止的点。您必须决定在哪里保存/恢复寄存器；修改“结构线程”以保存寄存器是一个不错的计划。您需要在“线程调度”中添加对“线程切换”的调用；您可以向“thread_switch”传递所需的任何参数，但目的是从线程“t”切换到“next_thread”。

Some hints:

- `thread_switch` needs to save/restore only the callee-save registers. Why?

- You can see the assembly code for uthread in user/uthread.asm, which may be handy for debugging.

- To test your code it might be helpful to single step through your thread_switch using riscv64-linux-gnu-gdb

  . You can get started in this way:

  ```
  (gdb) file user/_uthread
  Reading symbols from user/_uthread...
  (gdb) b uthread.c:60
  ```

  This sets a breakpoint at line 60 of `uthread.c`. The breakpoint may (or may not) be triggered before you even run `uthread`. How could that happen?

  Once your xv6 shell runs, type "uthread", and gdb will break at line 60. If you hit the breakpoint from another process, keep going until you hit the breakpoint in the `uthread` process. Now you can type commands like the following to inspect the state of `uthread`:

  ```
    (gdb) p/x *next_thread
  ```

  With "x", you can examine the content of a memory location:

  ```
    (gdb) x/x next_thread->stack
  ```

  You can skip to the start of `thread_switch` thus:

  ```
     (gdb) b thread_switch
     (gdb) c
  ```

  You can single step assembly instructions using:

  ```
     (gdb) si
  ```

  On-line documentation for gdb is [here](https://sourceware.org/gdb/current/onlinedocs/gdb/).
  ![pics\屏幕截图 2023-04-11 150747-16826872874861.png](pics%5C屏幕截图%202023-04-11%20150747-16826872874861.png)

​	首先介绍一下，有的东西。通过宏定义定义了线程的状态，分别是FREE，RUNNING，RUNNABLE。FREE指未分配，RUNNING正在运行，RUNNABLE可以运行。然后定义了栈的大小，定义了最多是4个线程MAX_THREAD。定义了结构体thread 包含栈和state。定义了全局变量all_thread[MAX_THREAD]存放所有的进程，current_thread存放当前运行的进程。

​	thread_init函数去初始化all_thread[0]，此线程为main函数所在线程。通过初始化state和设置current_thread完成。

​	thread_schedule 负责找到下一个可以RUNNABLE的线程，然后切换。

​	thread_create 创建一个线程，代码中给我们的是更改状态，需要我们补充代码。

​	thread_yield 更改状态，然后调用thread_schedule 切换线程。

​	thread_b/a/c，首先等待其他都启动成功，启动成功设置a_started为1，等待都为1，然后for循环打印输出，每次循环都会调用thread_yield去放弃cpu进行调度。

​	main函数负责初始化并创建线程，然后放弃处理进入调度。

​	我们需要补充的代码如下，thread_create，thread_schedule thread_switch以及thread的定义。

​	首先思考thread这个state需要保存什么？其实就是如何切换一个线程的问题，根据内核中切换线程的规则我们知道需要保存什么？内核的swtch函数的调用者默认swtch函数会做修改，所以调用者已经在自己的栈上保存了这些寄存器，当函数返回时，这些寄存器会自动恢复。所以swtch函数里只需要保存Callee Saved Register就行。所以寄存器分为两部分 Callee caller，callee被swtch保存，caller如果用到了则被保存在函数栈上，所以修改thread结构体如下。

```c
struct context
{
  uint64 ra;
  uint64 sp;

  // callee-saved
  uint64 s0;
  uint64 s1;
  uint64 s2;
  uint64 s3;
  uint64 s4;
  uint64 s5;
  uint64 s6;
  uint64 s7;
  uint64 s8;
  uint64 s9;
  uint64 s10;
  uint64 s11;
};
struct thread
{
  char stack[STACK_SIZE]; /* the thread's stack */
  int state;              /* FREE, RUNNING, RUNNABLE */
  char *name;				 /* for debug */
  struct context context;
};

```

​	那么thread_create，补充的代码便是 

```c
t->context.sp = (uint64)(t->stack+STACK_SIZE);
 t->context.ra = (uint64)*func*;
```

​	有人可能有疑问，struct thread 里面已经有了一个stack变量，为什么还要继续的想context中放入一个sp。因为stack只是去申请了这个stack这片空间，也就是这个线程的stack空间是stack这个变量，但是计算机时从sp向下访问，所以多设个sp寄存器，里面保存这个stack的最高地址。返回地址设为函数，这样thread_switch中的ret便会跳到对应的地方执行。不需要pc，只需要知道ra，ra会被填入pc。

结构体已经声明好了，那么thread_switch函数功能就是将当前寄存器的值保存在当前进程的结构体中，也就是从寄存器到结构体。然后将将要运行的进程的context恢复到寄存器中即可。

```assembly
	.text

	/*
         * save the old thread's registers,
         * restore the new thread's registers.
         */

	.globl thread_switch
thread_switch:
	/* YOUR CODE HERE */
		 sd ra, 0(a0)
        sd sp, 8(a0)
        sd s0, 16(a0)
        sd s1, 24(a0)
        sd s2, 32(a0)
        sd s3, 40(a0)
        sd s4, 48(a0)
        sd s5, 56(a0)
        sd s6, 64(a0)
        sd s7, 72(a0)
        sd s8, 80(a0)
        sd s9, 88(a0)
        sd s10, 96(a0)
        sd s11, 104(a0)

        ld ra, 0(a1)
        ld sp, 8(a1)
        ld s0, 16(a1)
        ld s1, 24(a1)
        ld s2, 32(a1)
        ld s3, 40(a1)
        ld s4, 48(a1)
        ld s5, 56(a1)
        ld s6, 64(a1)
        ld s7, 72(a1)
        ld s8, 80(a1)
        ld s9, 88(a1)
        ld s10, 96(a1)
        ld s11, 104(a1)
        
        
	ret    /* return to ra */

```

 	最后看thread_schedule，继续用完成保存即可。函数调用的时候会自动根据sp保存在stack中。

```c
// 进程切换
  if (current_thread != next_thread)
  { /* switch threads?  */
    next_thread->state = RUNNING;
    t = current_thread;
    current_thread = next_thread;
    
    thread_switch((uint64)(&(t->context)), (uint64)(&(current_thread->context)));
  }
```

### Using threads ([moderate](https://pdos.csail.mit.edu/6.828/2022/labs/guidance.html))

In this assignment you will explore parallel programming with threads and locks using a hash table. You should do this assignment on a real Linux or MacOS computer (not xv6, not qemu) that has multiple cores. Most recent laptops have multicore processors.

This assignment uses the UNIX `pthread` threading library. You can find information about it from the manual page, with `man pthreads`, and you can look on the web, for example [here](https://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_mutex_lock.html), [here](https://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_mutex_init.html), and [here](https://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_create.html).

The file `notxv6/ph.c` contains a simple hash table that is correct if used from a single thread, but incorrect when used from multiple threads. In your main xv6 directory (perhaps `~/xv6-labs-2021`), type this:

```
$ make ph
$ ./ph 1
```

Note that to build `ph` the Makefile uses your OS's gcc, not the 6.S081 tools. The argument to `ph` specifies the number of threads that execute put and get operations on the the hash table. After running for a little while, `ph 1` will produce output similar to this:

```
100000 puts, 3.991 seconds, 25056 puts/second
0: 0 keys missing
100000 gets, 3.981 seconds, 25118 gets/second
```

The numbers you see may differ from this sample output by a factor of two or more, depending on how fast your computer is, whether it has multiple cores, and whether it's busy doing other things.

`ph` runs two benchmarks. First it adds lots of keys to the hash table by calling `put()`, and prints the achieved rate in puts per second. The it fetches keys from the hash table with `get()`. It prints the number keys that should have been in the hash table as a result of the puts but are missing (zero in this case), and it prints the number of gets per second it achieved.

You can tell `ph` to use its hash table from multiple threads at the same time by giving it an argument greater than one. Try `ph 2`:

```
$ ./ph 2
100000 puts, 1.885 seconds, 53044 puts/second
1: 16579 keys missing
0: 16579 keys missing
200000 gets, 4.322 seconds, 46274 gets/second
```

The first line of this `ph 2` output indicates that when two threads concurrently add entries to the hash table, they achieve a total rate of 53,044 inserts per second. That's about twice the rate of the single thread from running `ph 1`. That's an excellent "parallel speedup" of about 2x, as much as one could possibly hope for (i.e. twice as many cores yielding twice as much work per unit time).

However, the two lines saying `16579 keys missing` indicate that a large number of keys that should have been in the hash table are not there. That is, the puts were supposed to add those keys to the hash table, but something went wrong. Have a look at `notxv6/ph.c`, particularly at `put()` and `insert()`.

Why are there missing keys with 2 threads, but not with 1 thread? Identify a sequence of events with 2 threads that can lead to a key being missing. Submit your sequence with a short explanation in answers-thread.txt

To avoid this sequence of events, insert lock and unlock statements in `put` and `get` in `notxv6/ph.c` so that the number of keys missing is always 0 with two threads. The relevant pthread calls are:

```
pthread_mutex_t lock;            // declare a lock
pthread_mutex_init(&lock, NULL); // initialize the lock
pthread_mutex_lock(&lock);       // acquire lock
pthread_mutex_unlock(&lock);     // release lock
```

You're done when `make grade` says that your code passes the `ph_safe` test, which requires zero missing keys with two threads. It's OK at this point to fail the `ph_fast` test.

Don't forget to call `pthread_mutex_init()`. Test your code first with 1 thread, then test it with 2 threads. Is it correct (i.e. have you eliminated missing keys?)? Does the two-threaded version achieve parallel speedup (i.e. more total work per unit time) relative to the single-threaded version?

There are situations where concurrent `put()`s have no overlap in the memory they read or write in the hash table, and thus don't need a lock to protect against each other. Can you change `ph.c` to take advantage of such situations to obtain parallel speedup for some `put()`s? Hint: how about a lock per hash bucket?

Modify your code so that some `put` operations run in parallel while maintaining correctness. You're done when `make grade` says your code passes both the `ph_safe` and `ph_fast` tests. The `ph_fast` test requires that two threads yield at least 1.25 times as many puts/second as one thread.

```c
static 
void put(int key, int value)
{
  int i = key % NBUCKET;

  // is the key already present?
  struct entry *e = 0;
  pthread_mutex_lock((lock+i));       // acquire lock
  
  for (e = table[i]; e != 0; e = e->next) {
    if (e->key == key)
      break;
  }
     
  if(e){
    // update the existing key.
    e->value = value;
  } else {
    // the new is new.
    insert(key, value, &table[i], table[i]);
  }
  pthread_mutex_unlock((lock+i));
}
```

​	解释一下为什么会出现访问冲突，主函数生成100000个随机数作为keys，然后开多线程去构造table，将key均分为n份，n指的是线程数量，然后调用put函数去生成table。put函数先用key对NBUCKET取模，来决定挂到那个头节点下面。然后遍历到该链表尾部进行插入操作。在遍历到尾部时，不同线程的可能在该尾部进行插入。所以当前的尾部不一定是最终的尾部，所以链表会在这分叉导致部分元素无法获得。所以构造五个锁，当进行遍历读取的时候获取锁，在插入的时候也需要获得锁。所以理论上最多时5倍速度，100个线程也得在5个锁后面排队。make grade时注意电脑配置，一开始用的垃圾服务器，跑不过ph_fast。

### Barrier([moderate](https://pdos.csail.mit.edu/6.828/2022/labs/guidance.html))

In this assignment you'll implement a [barrier](http://en.wikipedia.org/wiki/Barrier_(computer_science)): a point in an application at which all participating threads must wait until all other participating threads reach that point too. You'll use pthread condition variables, which are a sequence coordination technique similar to xv6's sleep and wakeup.

You should do this assignment on a real computer (not xv6, not qemu).

The file `notxv6/barrier.c` contains a broken barrier.

```
$ make barrier
$ ./barrier 2
barrier: notxv6/barrier.c:42: thread: Assertion `i == t' failed.
```

The 2 specifies the number of threads that synchronize on the barrier ( `nthread` in `barrier.c`). Each thread executes a loop. In each loop iteration a thread calls `barrier()` and then sleeps for a random number of microseconds. The assert triggers, because one thread leaves the barrier before the other thread has reached the barrier. The desired behavior is that each thread blocks in `barrier()` until all `nthreads` of them have called `barrier()`.

Your goal is to achieve the desired barrier behavior. In addition to the lock primitives that you have seen in the `ph` assignment, you will need the following new pthread primitives; look [here](https://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_cond_wait.html) and [here](https://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_cond_broadcast.html) for details.

```
pthread_cond_wait(&cond, &mutex);  // go to sleep on cond, releasing lock mutex, acquiring upon wake up
pthread_cond_broadcast(&cond);     // wake up every thread sleeping on cond
```

Make sure your solution passes `make grade`'s `barrier` test.

`pthread_cond_wait` releases the `mutex` when called, and re-acquires the `mutex` before returning.

We have given you `barrier_init()`. Your job is to implement `barrier()` so that the panic doesn't occur. We've defined `struct barrier` for you; its fields are for your use.

There are two issues that complicate your task:

- You have to deal with a succession of barrier calls, each of which we'll call a round. `bstate.round` records the current round. You should increment `bstate.round` each time all threads have reached the barrier.
- You have to handle the case in which one thread races around the loop before the others have exited the barrier. In particular, you are re-using the `bstate.nthread` variable from one round to the next. Make sure that a thread that leaves the barrier and races around the loop doesn't increase `bstate.nthread` while a previous round is still using it.

Test your code with one, two, and more than two threads.

```c
static int arrive_thread = 0;
static void barrier()
{
  pthread_mutex_lock(&bstate.barrier_mutex);
  arrive_thread++;
  if (nthread != arrive_thread)
  {
    pthread_cond_wait(&bstate.barrier_cond, &bstate.barrier_mutex);
    pthread_mutex_unlock(&bstate.barrier_mutex);
  }
  else
  {
    bstate.round++;
    arrive_thread=0;
    pthread_mutex_unlock(&bstate.barrier_mutex);
    pthread_cond_broadcast(&bstate.barrier_cond);  
  }
}
```

每个进程进来的时候获得锁，然后增加arrive_thread，当arrive_thread不等于线程数的时候需要wait，等待所有进程到达。等到所有进程都到达arrive_thread等于线程数目的时候，增加一轮，arrive_thread置0，便于下一次使用，然后释放锁，唤醒所有等待进程，被唤醒的进程会获得锁，然后释放。